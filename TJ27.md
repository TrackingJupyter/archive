Welcome to this twenty-seventh edition of the Tracking Jupyter newsletter (#TJ27).  
  
In parts, it's particularly opinionated, in others, contains dodgy cultural references, but I make no apologies _\[at times, it's a slog putting usefully concise items together; just waffling on and making spurious comments is much easier! —Ed.\]_.

News
----

This should have made it into #TJ26, but I missed a step on my beat... **Jupyter Enterprise Gateway**, which is now _"no longer dependent on Jupyter Kernel Gateway"_, is available at [v2.1](http://github.com/jupyter/enterprise_gateway/releases/tag/v2.1.0). The most notable new feature is support for [dynamic reconfiguration](https://jupyter-enterprise-gateway.readthedocs.io/en/latest/config-options.html#dynamic-configurables), which means you can now enable debug logging or adjust the maximum number of kernels per user without having to restart Enterprise Gateway.  
  
Jupyter Client is now out at [v6.0.0](https://discourse.jupyter.org/t/jupyter-client-6-0-0/3414), with some asynchronous execution support _\[interesting demos, anyone? —Ed.\]_. Python 3.8 is now supported, and Python 2.7 support has gone... _\[Gone...2.7y, gone? Poor, poor 2.7y... Dear, dear 2.7y... [etc.](https://www.youtube.com/watch?v=un8uQ3_fyYs)—Ed.\]_  
  
**IBM** just opensourced their **[elyra-ai/elyra](https://github.com/elyra-ai/elyra)** toolkit, that _"__extends JupyterLab Notebooks with an AI centric approach"_ _\[yeah. me neither...—Ed.\]_. The extensions suite includes a "Notebook Pipelines" visual editor (initially limited to building Kubeflow Pipelines); the ability to run a notebook as a batch job on a pipeline; Jupyter Enterprise Gateway integration, so you can choose where your code is run; _"Python Scripts as first-class citizens"_, which is to say: you can run your py files; and git integration (including a diff viewer), as well as sundry other items.  
  
Kubernetes is largely Greek to me, but these two items perhaps relate to the previous one? Firstly, **[Kale](https://github.com/kubeflow-kale/kale)** allows you to _"convert a JupyterNotebook to a Kubeflow Pipeline deployment"_ and, as described [here](https://medium.com/kubeflow/automating-jupyter-notebook-deployments-to-kubeflow-pipelines-with-kale-a4ede38bea1f), allows you to _"define pipelines by annotating Notebook code cells"_ and, via a deployment button in the Jupyter UI, _"convert the Notebook to a valid Kubeflow Pipelines deployment, resolve data dependencies and manage the pipeline’s lifecycle"_. Secondly, **Kubeflow**, [billed as](https://medium.com/kubeflow/kubeflow-1-0-cloud-native-ml-for-everyone-a3950202751) _"Cloud-Native ML for Everyone"_, has been released at v1.0. Providing "_a core set of stable applications needed to develop, build, train, and deploy models on Kubernetes efficiently"_, it also comes with a [justification](https://www.kubeflow.org/docs/notebooks/why-use-jupyter-notebook/) for using Jupyter notebooks as part of the process. The Kubeflow UI allows users to launch notebooks against pre-built docker images and selected CPU/GPU configurations. A [notebook-controller](https://github.com/kubeflow/kubeflow/tree/master/components/notebook-controller) allows users to create a custom resource "Notebook" according to a PodSpec associated with each Jupyter notebook. _\[I have absolutely no idea what that means or whether I paraphrased it meaningfully...;-)—Ed.\]_  
  
A couple of job ads for folk interested in working on Jupyter infrastructure for who _do_ know what they're talking about _\[apols if the ads are past their due by dates by the time this issue of Tracking Jupyer goes out; it's still worth tracking and sharing awareness of such things, I think...—Ed.\]_. First up, a research software engineer post at GESIS, in Cologne, to provide _"site reliability engineering for GESIS Notebooks and related services , develop new features in collaboration with the Jupyter community, maintain servers and Kubernetes clusters, and write and maintain user as well as technical documentation"_. Secondly, a _"three year term [full stack research programmer](https://careers.umich.edu/job_detail/185039/full_stack_research_programmer) to support learning analytics research work centered around the Jupyter environment in a data science education context"_ at  _\[mumble, grumble... learning analytics aka student surveillance...—Ed.\]_. There is some context in this [review](https://aprilwang.me/assets/pubs/CHI20_Callisto.pdf) of [Callisto](https://github.com/littleaprilfool/callisto) _\[I have to admit, I find the repo user name off-putting...—Ed.\]_, an extension for linking notebook development to related, but separate, design discussions.  
  
Poking around, the _Callisto_ extension seems to implement a **Jupyter Chat** server that saves state via a realtime ShareDB database to a MongoDB backend, but I haven't made the effort to try to get a demo instance up and running for me to test myself. _\[By the by, I've seen several mentions of [ShareDB](https://github.com/share/sharedb) lately... Must be something in the air...—Ed.\]_. Perhaps complementing that, [jupyterhub-share-link](https://github.com/danielballan/jupyterhub-share-link) is an experimental project that allows a user on the same JupyterHub server to temporarily share a link to one of their notebooks with another user on the same server. Clicking the shared link copies the last saved version of the sender's notebook to the recipient, along with container details in a DockerSpawner environment that will launch the appropriate environment. _\[I really do need to set up a test JupyterHub environment server somewhere, don't I?! —Ed.\]_   
  
Over at **Nextjournal**, the latest [changelog](https://nextjournal.com/changelog) reports _"[improved Jupyer Display support](https://nextjournal.com/blog/jupyter-display)"_, which includes things like multiple results for a code cell or interactive progress bar components (try them out in the [jupyter-display-playground](https://nextjournal.com/nextjournal/jupyter-display-playground)) and an embedded audio player for audio format (MP4, WAV) cell outputs.  
  
The Bokeh charting library is [now out](https://medium.com/@bokeh/announcing-bokeh-2-0-647042d0d977) at v2.0. Support for Python 2.7 support has gone... _\[Gone...2.7y, gone? Poor, poor \[Stop it: just, stop it.. —MetaEd.\] —Ed.\]_. Along with a new multiple choice widget, improved (rewritten) DatePicker widget and new tile providers for OpenStreetMap and ESRI Imagery, there's also integrated _ipywidgets_ support [available](https://github.com/bokeh/bokeh/blob/master/sphinx/source/docs/user_guide/jupyter.rst#ipywidgets-outside-the-notebook), _if_ you wrap an _ipywidget_ in an _ipywidgets\_bokeh.IPyWidget_ _\[so I could get a lot of other Javascript widgets into Bokeh by wrapping the original widget with jp\_proxy\_widget and then wrapping that in the Boken IPyWidget wrapper?! Turtles widget wrappers all the way down... —Ed.\]_.  
  
In addition to their support for MyBinder, _[OVHCloud](https://www.ovhcloud.com/en-gb/)_ is now also host to [_nbviewer_](https://nbviewer.jupyter.org/), the online notebook previewer (until now hosted by Rackspace) that previews notebooks from any URL. The [announcement post](https://blog.jupyter.org/nbviewer-has-a-new-host-ovhcloud-28bcab6a7ab4) has a wealth of information about the deployment strategy, including notes and reflection on the migration from Docker to Kubernetes+Helm. So if you fancy hosting a version yourself, or want to know more about what's involved, it's well worth a read. _\[It's probably also worth mentioning [nbpreview](https://github.com/jsvine/nbpreview), a standalone, HTML based notebook previewer that can be handy if you don't have a Jupyter server to hand and do have a private notebook you can't, or shouldn't, view using nbviewer. FWIW, I [once had a go](https://blog.ouseful.info/2019/06/11/zero-to-notebook-with-notebook-js-thebelab-inspired-by-nbgrader/) at adding ThebeLab/Mybinder code execution support to it...—Ed.\]_  
  
In academic circles, proper **citation** of papers, and increasingly, code libraries, is an important part of the game. But servers? Over on [nanohub.org/tools/jupyter](https://nanohub.org/tools/jupyter), I notice this: _"Abstract: starts the Jupyter notebook server in your home directory. The server will be started from the loaded anaconda-6 module"_ followed by _"Researchers should cite this work as follows: (2020), "Jupyter Notebook," https://nanohub.org/resources/jupyter. (DOI: 10.21981/W6TE-1750)."_  
  
If you are hosting a notebook previewer in your own organisation (citeable, or otherwise...), you might want to consider custom templates. Tim Paine's [nbconvert\_templates](https://github.com/timkpaine/nbconvert_templates) provides _"a framework for customizing NBConvert templates and building reports"_, noting that whilst _"NBConvert's default templates are largely designed with academic styling"_, he is more interested in providing _"a collection of templates with industrial reports in mind, primarily for financial services"_.  
  
By the by, anyone else noticing that **MyBinder doesn't always work** for you? _\[I have... Ghostery seems offended by it. A workaround is to call a subdomain, such as gke.mybinder.com, direct...—Ed.\]_ If so, there's [a thread](https://discourse.jupyter.org/t/mybinder-org-launches-just-hang-are-you-using-an-ad-blocker-ghostery-privacybadger-etc/3511) on the official Jupyter discourse forums collecting issues...  
  
Just in case, here's a [handy roundup](https://veekaybee.github.io/2020/02/25/secrets/) of how to securely store configuration **credentials**, passwords and secrets _\[see what I did there trying to make things search discoverable?! —Ed.\]_ in a Jupyter Notebook (see also [#TJ16](https://github.com/TrackingJupyter/archive/blob/master/TJ16.md#keeping-notebook-secrets)).  
  
It may not be much, but it is capacity building: _netpreserve_, (which is to say, IIPC, the International Internet Preservation Consortium), have just [announced](http://netpreserve.org/projects/jupyter-notebooks-for-historians/) **funding** for a small digital humanities project across the British Library, the University of Canberra and the National Library of Australia & National Library of New Zealand on _"__Asking questions with web archives – introductory notebooks for historians"_.  
  
In passing, a few other things I meant to include in my all too brief [#TJ26](https://github.com/TrackingJupyter/archive/blob/master/TJ26.md#managing-notebook-packages-and-state) round up of Python package import management and organising tools: [reorder\_python\_imports](https://github.com/asottile/reorder_python_imports) enforces a single import per line with the aim of reducing merge conflicts, whereas [zimports](https://github.com/sqlalchemyorg/zimports) focusses on _"r__eformatting Python imports so that they can pass the flake8-import-order__"_ test. [importanize](https://github.com/miki725/importanize) also _"organises imports using either PEP8 or custom rules but it also preserves comments surrounding imports"_. _\[By the by, there's a [related thread](https://github.com/mwouts/jupytext/issues/432) on the Jupytext issue tracker trying to thrash out a robust workflow for linting files and tidying them up automagically... I will get round to doing my linter round up one day...—Ed.\]_.  
  
Exciting news, this one, and, something I've been looking for signs of for some time _\[eg [here](https://discourse.jupyter.org/t/using-jupyterhub-binderhub-to-launch-arbitrary-containers/704)...—Ed.\]_, [ideonate/jhsingle-native-proxy](https://github.com/ideonate/jhsingle-native-proxy), (discussion [here](https://discourse.jupyter.org/t/new-package-to-run-arbitrary-web-service-in-jupyterhub-jhsingle-native-proxy/3493)), a simple proxy that allows you to wrap an arbitrary containerised web app in a thin layer that talks to a JupyterHub server _as if_ it were a notebook server _\[I think...—Ed.\]_. Developed by Dan Lester, who also refashioned Kitematic as the "local Binderhub app" that is [ContainDS](https://containds.com/) \[[#TJ24](https://github.com/TrackingJupyter/archive/blob/master/TJ24.md#news-and-announcements)\], I'm of the mind this could be transformational in terms of making a case for JupyterHub in education: JupyterHub server now becomes a multi-user authenticating gateway to _any_ containerised web application, not just one fronted by a notebook server. _\[And yes, I know jupyter-serverproxy lets you do that via a notebook server. But this removes the need for the notebook server...—Ed.\]_ If you want to try it, there's [a Binderised example](https://github.com/danlester/binderhub-streamlit-native) that wraps _streamlit_. Dan has also been beavering away on an experimental [repo2dockerspawner-alternative-version](https://discourse.jupyter.org/t/repo2dockerspawner-alternative-version/3584) _\[erm, shoulda read [#TJ19](https://github.com/TrackingJupyter/archive/blob/master/TJ19.md#binderhub-recontextualised-several-ways), where I mentioned Yuvi Panda's [repo2dockerspawner](https://github.com/yuvipanda/repo2dockerspawner)...?!;-) —Ed.\]_ as [announced](https://discourse.jupyter.org/t/repo2dockerspawner-alternative-version/3584) on the official Jupyter discourse forum.  
  
A new Github Action driven **Jekyll powered notebook to Github Pages** blogging workflow was [announced](https://fastpages.fast.ai/fastpages/jupyter/2020/02/21/introducing-fastpages.html) to lots of social media acclaim recently: [fastai/fastpages](https://github.com/fastai/fastpages). One really nice feature is a setup Github Action that can personalise the README when someone uses the repo template to create their own repo _\[I appropriated the recipe in [this piece](https://github.com/psychemedia/openlearn-publish-test/) of Heath Robinson tinkering...—Ed.\]_. Make of it what you you will... _\[Personally, I think the top level directory is really messy and confusing, and generating tokens and accepting PRs is likely to alienate non-git/Github users. I prefer builds where the clutter is managed in a branch, such as I'm trying to do in this [Jupyter Book workflow](https://github.com/ouseful-testing/jupyter-book-autobuild)... —Ed.\]_  
  
Rival to Jekyll build routes are the myriad **Sphinx powered workflows**, of course, which reminds me that I still haven't tried out [jupinx](https://jupinx.quantecon.org/) _\[though I have [also been fighting](https://github.com/ouseful-testing/oer-md-publish-sphinx-actions) with nbsphinx powered Github Action workflows...—Ed.\]_. I should probably also pay more heed to Sphinx themes, such as this Bootstrap-based [pandas-sphinx-theme](https://github.com/pandas-dev/pydata-bootstrap-sphinx-theme) (as used by _pandas_, _bokeh_, and _JupyterHub_ for their documentation sites) or the experimental [sphinx-jupyter-book-theme](https://github.com/choldgraf/sphinx-jupyter-book-theme). It looks like there may be a wealth of Sphinx plugins too, such as [jupyter-sphinx](https://github.com/jupyter/jupyter-sphinx) \[[docs](https://jupyter-sphinx.readthedocs.io/en/latest/)\], which seems to support the rendering of Jupyter interactive widgets, and Chris Holdgraf's new-ish [sphinx-copybutton](https://github.com/choldgraf/sphinx-copybutton) \[[docs](https://sphinx-copybutton.readthedocs.io/en/latest/)\] which adds a "copy" button to code blocks so you can copy code from them. This [recipe](https://stanczakdominik.github.io/posts/simple-binder-usage-with-sphinx-gallery-through-jupytext/), and associated [demo gallery](http://docs.plasmapy.org/en/latest/auto_examples/index.html), for running Jupytext transformed code on MyBinder from a [sphinx-gallery](https://sphinx-gallery.github.io/stable/index.html) context also looks useful.  
​  
I keep looking for weak signals around folk developing their own custom Jupyter front-end UIs that make use of Jupyter services on the back end to support user code execution, my thinking being that the more people developing such services, the more baked in various elements of the Jupyter ecosystem become as a backend infrastructure. So here's one example that I spotted over the last week or two: [mljar/mljar--studio](https://github.com/mljar/mljar-studio) _\[which seems to have started redirecting to [herqooly/herqooly](https://github.com/herqooly/herqooly)? —Ed.\]_ that uses _"\[a\] Jupyter Notebook server for computation, \[a\] Django server to store needed information and manage projects, and react and redux in the frontend"_. It also incorporates a drag and drop dashboard builder to support custom report layouts. Another one is [koopera](https://github.com/rsn491/koopera), a new collaboration app _\[reminiscent of_ _[reviewnb](https://www.reviewnb.com/) and, to a lesser extent, [nbgallery](https://github.com/nbgallery/nbgallery), maybe? —Ed.__\]_ designed _"to help Data Science teams share and review their Jupyter notebooks"_. Notebooks can be imported from linked GitHub repositories and comments made at the code cell level.  
  
Plugins developed for other workbenches that install Jupyter servers are also becoming a thing. For example, [Home Assistant](https://www.home-assistant.io/hassio/), which _"turns your Raspberry Pi (or another device) into the ultimate home automation hub"_ has the community provided [addon-jupyterlab-lite](https://github.com/hassio-addons/addon-jupyterlab-lite) to add a JupyterLab server to the hassio-verse.  
  
Protocol driven Jupyter powered code execution is also increasingly being supported by IDEs. VSCode gets a lot of mentions in this context, _\[although arguably the Atom [Hydrogen](https://atom.io/packages/hydrogen) extension got there first, not to mention Spyder...—Ed.\]_ and now there's also [Helium](https://github.com/pykong/Helium), a **Sublime Text** 3 extension which provides _"in-editor code execution and autocomplete in interaction with Jupyter kernels"_. Speaking of **Spyder**, here's a collection of [spyder-kernels](https://github.com/spyder-ide/spyder-kernels) that _"can be launched either through Spyder itself or in an independent Python session to provide interactive or file-based execution of Python code inside Spyder"_. If the **[Kakaoune](https://kakoune.org/)** editor is more your thing, [neptyne](https://github.com/danr/neptyne) claims to offer _"editor-agnostic jupyter kernel interaction and kakoune integration"_.  
  
If a code editor is still too rich an environment for you, how about [jupytui](https://github.com/mosiman/jupytui), _"a terminal client for Jupyter notebooks"_, though still in the early stages of development, with a goal of _"eventually being able to have a terminal client that modifies notebooks and connects to the kernel"_.  
  
Complementing front end user developments, I also note the appearance of hosted services that support scheduled and parameterised notebook execution. This is interesting because it suggests the use of notebooks as code scripts used in production. [Papermill](https://github.com/nteract/papermill) is often used to support parameterised execution as part of a self-service approach (or from way back, [run\_jnb](https://github.com/hz-inova/run_jnb)), but what if you want to access such things _as a service_? New to me, Treebeard.io (_"we help data scientists ship"_) allow you to deploy notebooks to a  cloud server against custom built Docker images run _"on a schedule you define. Tagged output (artifacts) from the notebook are made available for download (API key required)"_ \[[repo](https://github.com/treebeardtech/hello_treebeard)\]. If you want to host your own UI fronted _papermill_ workflow, there's always [paperboy](https://github.com/timkpaine/paperboy).  
  
On the command line, [GitMiller](https://github.com/UtrechtUniversity/GitMiller) looks interesting, a tool that uses papermill to execute notebooks pulled from a Github repository. _\[Could be interesting to see this merged with repo2docker and nbgitpuller somehow? —Ed.\]_  
  
One of the things I had imagined myself doing when I started _Tracking Jupyter_ was getting my head into the **technology / innovation management** space and exploring how the Jupyter ecosystem was evolving when viewed under that lens. But I haven't found, or made, the time to start that reading, or start pondering such considerations in any depth. But other people are starting to. For example, albeit relatively informally, in this [blog post](https://ljvmiranda921.github.io/notebook/2020/03/06/jupyter-notebooks-in-2020/) on _How to use Jupyter Notebooks in 2020 (Part 1: The data science landscape)_.  
  
Anyone else get the feeling that getting started with **robot simulators** in a Jupyter context _\[indeed, in any context...—Ed.\]_ still seems daunting? This write up on [Robot development with Jupyter](https://medium.com/@wolfv/robot-development-with-jupyter-ddae16d4e688) that appeared right at the end of last year teases the ability of "just" being able to fire up a quick simple simulator using [amphion](https://github.com/rapyuta-robotics/amphion), a _three.js_ underpinned package for visualising ROS powered objects. But the demo seems to call on a couple of server endpoints, at which you point you think it probably isn't "just" that easy and you might have to spend an afternoon, or a weekend, or who knows how long, trying to figure out what those servers are and how on earth you're supposed to set them up...

The [recent  (v.9.6) )release](https://rapidminer.com/blog/power-of-jupyter-notebooks-python-rapidminer/) of the **RapidMiner** data science platform _\[I see a lot of ads for them in my Twitter stream...—Ed.\]_ now bundles a pre-configured JupyterHub instance, accessible via a single sign on from the RapidMiner Server user interface. Meanwhile, **Azure Cosmos DB** Jupyter notebooks are [now previewing](https://azure.microsoft.com/en-us/updates/new-features-in-azure-cosmos-db-jupyter-notebooks-are-now-in-preview/) new cell magics to support bulk data uploads and SQL queries onto the Azure Cosmos DB, and an integrated nteract data explorer _"for quick, built-in visualizations"_.  
  
We all know spreadsheets are still the _de facto_ app for many folk for working with simple _\[and not so simple...—Ed.\]_ macro powered models _\[including new ones, as well as legacy ones, come to that...—Ed.\]_, tabular datasets, informally structured spreadsheets-as-databases, and sinks for SQL queries on properly structured datsets, but there are already more than a few graphical tools out there wrapping pandas dataframes with interactive UIs, and now there's another one, this time resulting from a reimagination of a **SAS** based workflow: [man-group/dtale](https://github.com/man-group/dtale) _\[Man Group being an "active investment management firm" —Ed.\]_ is a _"Flask/React client for visualizing pandas data structures"_ \[[example](http://andrewschonfeld.pythonanywhere.com/dtale/main/1)\] resulting from _"a SAS to Python conversion of an original Perl script wrapper on top of SAS's insight function to a lightweight web client on top of Pandas data structures"_. In turn, this reminds me of [sassoftware/saspy](https://github.com/sassoftware/saspy), _"a \[cross-platform\]_ _Python interface module to SAS 9.4"_ that can connect to _"__local or remote Linux SAS, IOM SAS on Windows, Linux (Including Grid Manager), or MVS, and local PC SAS"_ or operate via a [sas\_kernel](https://github.com/sassoftware/sas_kernel). SAS queries are generated by the package, executed in the SAS environment and can be returned as _pandas_ dataframes. To get a feel for how the SAS thang works, here's a [tutorial](https://github.com/sascommunities/sas-global-forum-2019/tree/master/3133-2019-Gaines) on _"__Learning Data Science with SAS® University Edition and JupyterLab"_.  
  
Something else I'm starting to notice is the increasing **commodity nature** of Jupyter notebook and JupyterLab server environments. For example, data science platforms like [MatrixDS](https://matrixds.com/features/), which offer access to a range of applications in an integrated workbench environment, don't need to advertise the availability of Jupyter notebooks: they're presumably assumed. _\[Even in the [docs](http://userguide.matrixds.com/tools.html), you only see trace examples that JupyterLab, for example, is one of the provided services. And that JupyterLab is one of the glimpsed at services identifiable in the animated GIFs perhaps also says something? —Ed.\]_ In the public funded service environment, Jupyter also features heavily, such as in the [Whole Tale](https://wholetale.org/) platform, _"an NSF-funded Data Infrastructure Building Block (DIBBS) initiative to build a scalable, open source, web-based, multi-user platform for reproducible research enabling the creation, publication, and execution of tales - executable research objects that capture data, code, and the complete software environment used to produce research findings"_.  
  
Services that _do_ try to get some traction off the back of their Jupyter integration are still evident though _\[it's a bit like the Web 2.0 private beta whack-a-mole days all over again...—Ed.\]_. For example, [thefable.co](https://thefable.co/) \[[annoucncement](https://medium.com/@danilkolesnikov/launching-fable-ce62540b3b8a)\] claims to allow you to _"share and discover \[Jupyter and Zeppelin\] notebooks with fellow data scientists and non-technical stakeholders"_. _\[It's getting to be that there really aren't enough free hours for me in the weeks between Tracking Jupyter editions to try everything any more...—Ed.\]_  
  
Going back to simple Python packages you can play around with in a coffee break, I do like a nice funnel plot for seeing how normal things aren't... And heres' a new one, [johnhw/funnelplot](https://github.com/johnhw/funnelplot), offering "simple funnel plots in Python, using Matplotlib" _\[I prefer them oriented the other way, though...—Ed.\]_. Also matplotlib-y, I note that the [ipympl](https://github.com/matplotlib/ipympl) (jupyter-matplotlib) _ipywidget_ package, which supports interactive matplotlib features in notebooks and Jupyterlab, just requires a simple _%matplotlib widget_ magic invocation to get things up and running. And although not Jupyter related, but possibly of interest to folk who work with data and need to test things in a noisy environment: [Noisify](https://github.com/dstl/Noisify). As well as "machine" noise, it can also add "human error" noise (typos, transpositions etc.) to your test data sets. _\[By the by, I also did a [round-up](https://blog.ouseful.info/2019/02/18/generating-fake-data-quick-roundup/) once-upon-a-time of packages to support the generation of "fake", which is to say, synthetic, data sets...—Ed.\]_

Service Is, As Service Does
---------------------------

I'm not sure _how_ folk might be able to make use of this, but URL glue is always handy to have around. In particular, the **[jupyter-urlparams](https://github.com/manics/jupyter-urlparams)** extension allows you to pass a couple of URL parameters, _paramsfile_ and _params_, into a notebook server (such as one launched via MyBinder). In return, a file named after the value of the _paramsfile_ parameter will be created and the _params_ value written to it. You can then open the file from a notebook, for example, and load in the specified parameters from the newly created file.  
  
That can be handy in a user context, for example, when sharing a Binder link, but how about if we step away from _user_ interfaces for a minute and ask: are there any new components out there that will let us use notebooks to deliver machine-to-machine services?  
  
Perhaps one of the understated gems of the Jupyterverse, the [Jupyter Kernel Gateway](https://github.com/jupyter/kernel_gateway)  provides a way of [publishing a simple API service](https://jupyter-kernel-gateway.readthedocs.io/en/latest/http-mode.html) where the call handlers are defined as functions in a single Jupyter notebook _\[here's [my first attempt](https://blog.ouseful.info/2017/09/06/building-a-json-api-using-jupyer-notebooks-in-under-5-minutes/) at using it from way back when...—Ed\]_. The [jupyter-lab-serverless](https://github.com/u2takey/jupyter-lab-serverless) extension has a similar feel to it, though rather than having to running a standalone kernel gateway service, it looks like you can just create your service function calls in one notebook and then immediately start calling on them from another. _\[Hmm... could this provide a route to calling API services running in MyBinder. Cf. also [this hack](https://blog.ouseful.info/2019/12/10/accessing-mybinder-kernels-remotely-from-ipython-magic-and-from-vs-code/) for connecting to a MyBinder kernel from your own notebook server..\[Does anyone have a better / more reliable way of doing this?!—MetaEd.\].—Ed.\]_  
  
Working outside the Jupyter server context, at least when it comes to publishing the service, if not running it, [nb\_to\_html](https://github.com/skyline-ai/nb_to_html) is a simple Flask fronted service that runs against a service defined in a Jupyter notebook. Optional URL parameters are consumed in a _papermill_ mediated notebook execution step, and an HTML view of the run notebook, generated using _nbconvert_, provided as a service response. There a write up of how the pieces fit together [here](https://medium.com/@_orcaman/a-simple-serverless-reporting-endpoint-based-on-jupyter-notebooks-4cf3e1d15ebe). _\[I'm not sure what benefits it offers over something like Voilà, though? —Ed.\]_  
  
In passing, I note that papermill seems to crop up in all sorts of places. For example, it forms part of [notebook\_restified](https://github.com/kafonek/notebook_restified), which attempts to treat Jupyter Notebooks _"as stand-alone functions with input parameters and a return values \[for use as\] REST endpoints, or as callback functions within other Notebooks"_. With the server extension enabled, any notebook can be called as a REST endpoint by replacing the _/notebooks_ URL path element with _/restified_; parameters can be passed in as URL parameters via and HTTP GET or, using JSON encoded parameters, via an HTTP POST.

Jupyter Learning Environments...
--------------------------------

Over in edu land, a recent [show'n'tell video](https://zoom.us/rec/play/upMvduj-rj03SNyc5QSDUfRwW43oJqis0Xce-KdemU3jV3cAM1ryb-NDZuHDvD5JpfoL1CAZ9mVN9Q9g) from the _LibreTexts_ open textbooks platform folk shows off some of their [Jupyter integrations](https://libretexts.org/advanced.html). Starting at 0h48m in, the presentation starts with a review of the motivation, which is to say, code related interactive textbooks, at scale _\[and relevant for "coding to learn" as well as "learning to code", a mantra I'm hearing more and more...—Ed.\]_. As well as offering a Jupyterhub, with server proxied access to RStudio, LibreTexts textbooks can also run code via [ThebeLab](https://thebelab.readthedocs.io/) integration (eg 1h12m). The page level authoring environment (1h21m) allows authors to embed multiple interactive code cells within the page. The author can run the code and render both code and output in the published page, or optionally hide the code and/or code output.  
  
In passing, I note that the [NRELabs](https://nrelabs.io/) website (_"an open source educational project focused on making network automation and the emerging discipline of network reliability engineering"_) has had a recent refresh. This project develops the [Antidote](https://docs.nrelabs.io/antidote/antidote-architecture) learning environment, which offers the learner access to lessons published as a hosted VM. The split screen web UI contains instructional text on one side and tabbed command line consoles (as well as http/HTML endpoints served from inside the VM) on the other. Of note, one of the lessons ([Multi-Vendor Network Automation with NAPALM](https://go.nrelabs.io/labs/?lessonId=13&lessonStage=1)) uses a Jupyter notebook to provide the instructional material _\[in an earlier version of the platform at least, you could also see notebooks in the right hand panel...—Ed.\]_.  
  
_\[I've been meaning to do a round up of quiz, assessment and grading extensions for over a year now. I really should get round to it at some point... \[but: dull, dull, boring, dull..—MetaEd.\] —Ed.\]_

From Literate Programming to Reactive Computational Narratives 
---------------------------------------------------------------

I'm still not sure how the "**computational narrative**" approach complements "**literate programming**" _\[the former perhaps uses computation and computational outputs in support of a narrative, whereas the latter (literate computing) uses narration to describe the computation, perhaps? Fernando Perez and Stephen Wolfram, amongst others, have written variously about this, I think...—Ed.\]_ but I think it's worth bearing in mind that these may represent different approaches, or _attitudes_, towards publications that blend text, code and potentially code outputs. And different tools may be necessary to best support each of them...  
  
In terms of mechanics, the [introductory docs](https://nbviewer.jupyter.org/github/deathbeds/pidgy/blob/master/index.md.ipynb) for **[pidgy](https://github.com/deathbeds/pidgy)**, which bills itself as _"IPython for literate programming of computational essays"_ _\[hmmm...—Ed.\]_, using code-embedding markdown, identifies two essential steps within a literate programming, _qua_ documentation production, publishing route: _Tangle_, which translates the documentation language (such as markdown) into the programming language, and _Weave_, where computational objects in the text (for example, code variables embedded in text) are transformed to literal values. For example, _pidgy_ _"provides the ability include representation of live programming objects directly in the Markdown using jinja2 syntax_", although we might also consider the rendering of code cell outputs (tables etc.) as part of the _Weave_ step. _\[I'm not sure if similar steps were articulated in Knuth's original [WEB literate programming](http://www.literateprogramming.com/) workflow? —Ed.\]_ Between those two steps, an interstitial _Test_ step tests (and maybe lints?) the code, presumably reporting any failures into the rendered document. _\[The "Weave" reference also reminded me of publishing tools like [Sweave](https://support.rstudio.com/hc/en-us/articles/200552056-Using-Sweave-and-knitr), which allows R code execution as part of a LaTeX publishing workflow and [dexy.it](http://www.dexy.it/), which was my first real intro to the code-generating-text-documents publication thang...—Ed.\]_  
  
And speaking of "tangle", anyone remember Bret Victor's **[tangle.js](http://worrydream.com/Tangle/)**, a small Javascript library for developing reactive documents? _\[If you aren't familiar with Bret Victor's work, I strongly urge you to watch his [talk](https://vimeo.com/36579366) on "__Inventing on Principle"__...—Ed.\]_ A couple of nice features that I miss are "sliding numbers", where you click on a rendered numerical value in the text and drag it to change it, and "toggleable enumerated list" words _\[words are failing me, at the moment! —Ed.\]_, where you click on a word-variable to change its value. Behind the scenes, these variables could be used as inputs to other reactive components. So for example, in the text you might have a phrase _"with a mortgage interest rate of X%"_ linked to a chart of accumulated mortgage repayments over time, and changing the number directly _in the text_ reactively updates the chart. Anyway, "tangle-text" _ipywidgets_ _do not_, as far as I know, yet exist, other than as the long since bit-rotted [ipytangle](https://github.com/bollwyvl/ipytangle), a very early proof of concept _\[Anyone? Anyone?! [etc](https://www.youtube.com/watch?v=uhiCFdWeQfA).—Ed.\]_. But over in Observable notebook land, you _can_ now start to play with them: [Observable Tangle](https://observablehq.com/@mbostock/tangle).  
  
The **Observable** folk also seem to be reaching out to the Jupyter community at the moment, or maybe looking to convert Jupyter users, noting that _"\[t\]he most common reason for Jupyter users to use Observable is creation and distribution of visualizations"_. For example, [Observable for Jupyter Users](http://https://observablehq.com/@observablehq/observable-for-jupyter-users) reviews some of the major differences between Jupyter and Obervable notebooks: firstly, only a single programming language is supported, and that is a superset of JavaScript _\[so I keep wondering... Pyodide, WebAssembly and a scipy stack in your browser...—Ed.\]_; secondly, cells can be named, with cell outputs  shown above the code; the code can be revealed or hidden, and produces a single output value; the scope of variables defined within a code block is limited to that block, with variables defined outside code blocks global in scope; cells can also be imported from other notebooks; finally, keyboard shortcuts may differ in their action... The most notable difference (apparently...) is the _reactive_ nature of Observable cells, _qua_ spredsheet cells: _"Observable keeps track of which cells depend on the output of other cells and recalculates results as necessary to keep everything up to date_" which means that global variables can only be defined once, and not reassigned, and circular definitions are banned. That said, cells can be presented in any order. Observable notebooks also live on the web and the code runs in the browser. A second part of the current advocacy project is a [worked example](https://observablehq.com/@observablehq/visualize-a-data-frame-with-observable-in-jupyter) of visualising a dataframe with observable in a Jupyter notebook. _\[I can't help but wonder if the Observable code could be wrapped as a Jupyter widget using_ _[jp\_proxy\_widget](https://github.com/AaronWatters/jp_proxy_widget)... Anybody fancy trying it?!—Ed.\]_  
  
And finally, when it comes to "documentation languages", the [Executable Book Project](https://ebp.jupyterbook.org/en/latest/) is developing an extended markdown format — [MyST](https://myst-parser.readthedocs.io/en/latest/), "Markedly Structured Text" — to support publication of _"professoinal computational narratives_" _\[I smiled...:-) But this project is still flying under the radar and hasn't been formally announced yet, I think... —Ed.\]_. The initial publication route seems to be via Sphinx using [MyST-NB](https://myst-nb.readthedocs.io/en/latest/), _"a Sphinx parser for ipynb files"_. The format will [hopefully](http://github.com/mwouts/jupytext/issues/447) also join the list of Jupytext supported formats, which can currently roundtrip cell inputs, although not typically code outputs.  
  
The _ExecutableBookProject_ Github organisation is also home to [jupyter-cache](https://github.com/ExecutableBookProject/jupyter-cache), which looks likes it intended to be able to cache workflow steps at the notebook cell level, store external assets and execution artifacts, support parallel notebook execution and log execution statistics and reports. _\[Some similarity to elements of [nbexplode](https://github.com/takluyver/nbexplode) there, perhaps? —Ed.\]_ Complementing that idea in certain respects, I think, the [Jupyter-Notebooks-Tags-Extension](https://github.com/hi917/Jupyter-Notebooks-Tags-Extension) _"provides the ability to run code cells using tags"_; all code cells with tags matching an input tag list are added to an execution list and then executed. Perhaps interesting implementation-wise, the tags extension _"uses localStorage to store metadata and data about the tag strings"_. And then there are complete cell revision history tools (eg as described in [#TJ24](https://github.com/TrackingJupyter/archive/blob/master/TJ24.md#keeping-track-of-cell-history)), which presumably also factor in here too, including this new to me one, [jupyterlab\_autoversion](https://github.com/timkpaine/jupyterlab_autoversion), which seems to offer _"enhanced checkpoints, and versioned and persistent between restarts on every save"_.  
  
_PS I don't think it's been published yet, but as I found this and you could too, there's a sneak preview of the forthcoming JupyterCon 2020 website available from [the site repo](https://github.com/jupytercon/jupytercon.github.io)..._  
 

* * *

Disclaimer: this newsletter is produced independently of the official Jupyter project.

_All errors are down to the editor... Oops.. If I make any that I later become aware of, or I'm informed of, I'll announce them in the first possible issue thereafter. If it's really bad, I'll do a Stop Press/emergency issue._

If you have any Jupyter related news items or notebooks you'd like to be considered for inclusion in the newsletter, or experiences of using any of the technologies described in this newsletter that you'd like to share, please email them along to: tony.hirst@open.ac.uk
