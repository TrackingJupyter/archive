Welcome to this twenty-fifth edition of the _Tracking Jupyter_ newsletter (#TJ25).  
  
As it's the first newsletter of the year, I _could_ start with some some predictions, but who knows what's going to happen over the next twelve months... _\[Erm....?! —Ed.\]_ One thing I do note is that this newsletter seems to be getting overly (personally) opinionated... I'll try to rein that back in future issues...  
  
As it is, there's the usual eclectic collection of news and announcement items, as well sections on running JupyterHub and BinderHub on GPU and FPGA powered servers, a look at how the Binderverse is evolving,  and some ideas that are out there about logging, diffing and managing cell execution in notebook environments. There are also some notes on preservation _\[which is to say, long term reproducibility—Ed.\]_ issues _\[best to get in there early...—Ed.\]_.  
  
So let's get to it...

News and Announcements
----------------------

Over on the official Jupyter blog, an [invited post](https://blog.jupyter.org/gesis-joins-the-binder-federation-8603d878ff77) described GESIS' recent inclusion in the BinderHub federation. _\[So who's gonna sign up next..?:-)—Ed.\]_ There's also a [review of 2019](https://blog.jupyter.org/a-2019-retrospective-from-the-binder-project-57a449517362) from the BinderHub team.    
  
Related event-wise, if you're passing **Cambridge, UK**, on Wednesday, January 29, 2020, the 16th PyData Cambridge meetup will feature a talk on _"Reproducible Research: An Introduction to The Turing Way and Binder"_ from the Turing Institute's Dr Sarah Gibson, followed by a _"From Zero to Binder!"_ hands-on tutorial.  
  
**nbformat**, which provides the notebook _.ipynb_ format specificaiton, is [now](https://discourse.jupyter.org/t/nbformat-5-0-1-release/3001) officially at v0.5.1.  
  
Also on the Jupyter blog, a [recent announcement](https://blog.jupyter.org/voil%C3%A0-is-now-an-official-jupyter-subproject-87d659583490) on revealed that _Voilà is now an official Jupyter sub-project_. If you haven't played with it, Voilà makes it easy to publish Jupyter powered web-based applications and slideshows from nbformat-compliant documents. _\[An alternative is offered by [appmode](https://github.com/oschuett/appmode); I'm not sure if there are situations where appmode is prerefable to Voilà?—Ed.\]_ I also notice [this post](https://medium.com/@limpford/how-to-create-an-interactive-dashboard-in-python-using-holoviz-panel-2de350b6d8df) on _How to Create an Interactive Dashboard in Python Using HoloViz \[PyViz, as was...—Ed.\] Panel_. A nice feature of this is the ability to simply embed data into the dashboard so an HTML export will continue to work _\[does voilà support this sort of thing too? I've never had much joy keeping widgetised HTML exported notebook pages working...—Ed.\]_;  a hide code / show code toggle button allows you to publish code but keep it discreet.  
  
Another, rather new looking _\[keep up, keep up! —Ed.\]_, dashboarding framework, [jupyter-flex](https://github.com/danielfrg/jupyter-flex) seems to build on Voilà to provide "easy interactive dashboards, using Markdown headers and Jupyter Notebook cell tags to define the dashboard components". Row and column based layouts are supported, with Voilà and _ipywidgets_ providing interactive support and _nbconvert_ generating static reports.  
  
The [latest](https://devblogs.microsoft.com/python/python-in-visual-studio-code-january-2020-release/) (January 2020) **VS Code Python extension** update supports Jupyter kernel selection (along with kernel status) and local images in markdown \[[changelog](https://github.com/Microsoft/vscode-python/blob/master/CHANGELOG.md#202010-6-january-2020)\]. Visual Code users might also be interested in [this tip](https://blog.jupyter.org/connect-to-a-jupyterhub-from-visual-studio-code-ed7ed3a31bcb) that appeared on the official Jupyter blog showing how to connect to JupyterHub from Visual Studio Code _\[and which in turn prompted me to see if I could connect VS Code to a BinderHub instance. TLDR: [you can, sort of](https://blog.ouseful.info/2019/12/10/accessing-mybinder-kernels-remotely-from-ipython-magic-and-from-vs-code/)...—Ed.\]_.  
  
Elsewhere, support for the Jupyter notebook format is [now available](https://cantor.kde.org/2019/12/23/cantor-19-12.html) with the v.19.12 release of the new to me [Cantor](https://edu.kde.org/cantor/) application _\[what took you so long?! —Ed.\]_. Cantor is a KDE Frontend to mathematical applications _\[so a Linux desktop app, then...—Ed.\]_, which supports several backends (Julia, KAlgebra, Lua, Maxima, Octave, Python 2 and 3, Qalculate, R, Sage, and Scilab) using an extensible plugin mechanism. [Jupyter support](http://sirgienkogsoc2019.blogspot.com/2019/08/cantor-and-support-for-jupyter.html) includes the ability  to open and save _.ipynb_ files as well as round-trip between Cantor’s native worksheet format and Jupyter notebooks. In passing, I also [note](https://wiki.sagemath.org/Python3-Switch) that _"starting from version 9.0 released in January 2020, SageMath now runs on top of Python 3"_. Despite being essential for interoperability, I further note that the _.ipynb_ file format is _not_ listed in the in the Library of Congress _Sustainability of Digital Formats_[format list](https://www.loc.gov/preservation/digital/formats/fdd/browse_list.shtml), which is one place you might go to look up where to find normative references for filetype specifications.  
  
On the matter of desktop notebook clients, Cantor brought to mind Stencila \[[#TJvarious](https://github.com/TrackingJupyter/archive/search?q=stencila&unscoped_q=stencila)\] and what sort of state it might be in. From the website, I can't tell, but I did come across this related project, [encoda](https://github.com/stencila/encoda), _"codecs for structured, semantic, composable, and executable documents"_ for converting between document types, _.ipynb_ included.  
  
Something I forgot to mention in the round-up of AWS _re:Invent_ announcements in [#TJ24](https://github.com/TrackingJupyter/archive/blob/master/TJ24.md#news-and-announcements) was that the new AWS **quantum computing** stack, [Amazon Braket](https://aws.amazon.com/blogs/aws/amazon-braket-get-started-with-quantum-computing/), which lets you _"__build and test your circuits in a simulated environment and then run them on an actual quantum computer"_ _\[I...just...what?! —Ed.\] _is fronted by Jupyter notebooks.  
  
Not strictly a Jupyter thing, Netflix [recently](https://medium.com/netflix-techblog/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9) opensourced their [metaflow](https://docs.metaflow.org/) Python framework for managing data science workflows. Workflows are described as sequence of steps, each comprised of arbitrary Python code, in a simple directed acyclic graph. Interestingly from a Jupyter perspective, _"UI efforts \[have focussed\] on a seamless integration with notebooks, instead of providing a one-size-fits-all Metaflow UI. \[D\]ata scientists can build custom model UIs in notebooks, fetching artefacts from Metaflow, which provide just the right information about each model"_.   
  
Also workflow related, [WIPP](https://github.com/usnistgov/wipp), the web image processing pipeline is _"an open-source web-based algorithmic plugin platform for trusted image-based measurements from terabyte-sized images developed at the National Institute of Standards and Technology (NIST), in collaboration with the National Institutes of Health (NIH) - National Center for Advancing Translational Science (NCATS)"__\[hat tipping public bodies is an important thing, I think, in the current age of populist bonkers politics... —Ed.\]_ I'm not sure how it works _\[or even if it works; I haven't checked...—Ed.\]_, but if you need Jupyter integration, it seems there's a [jupyterlab\_wipp](https://github.com/LabShare/jupyterlab-extensions/tree/master/jupyterlab_wipp) extension that might help.  
  
Complementing that, I just noticed AXA Group's **[Parsr](https://github.com/axa-group/Parsr/blob/master/demo/jupyter-notebook/parsr-jupyter-notebook.ipynb)**, a pipeline for extracting text and data from images and PDFs using a host of component parts ([demo notebook](https://github.com/axa-group/Parsr/blob/master/demo/jupyter-notebook/parsr-jupyter-notebook.ipynb)).  
  
As time goes by, and as I need to keep various legacy bits of code running, particularly scrapers, I'm starting to appreciate how various forms of testing might be useful. The [jupyterlab\_celltests](https://github.com/timkpaine/jupyterlab_celltests) extensions provides a convenient way of writing tests, on a cell by cell basis, for linearly executed notebooks. Additional notebook quality checks are also supported, such as reporting against the maximum number of lines per cell as well as cells, function definitions and class definitions per notebook. Linting checks are also available. _\[Loosely related, I've been sketching some of my own notebook quality checks [here](https://blog.ouseful.info/2019/12/17/fragment-metrics-for-jupyter-notebook-based-educational-materials/). I've also been [dabbling](https://blog.ouseful.info/2019/12/16/fragment-visualising-jupyter-notebook-structure/) with a simple notebook structure visualiser...—Ed.\]_  
  
When it comes to profiling code in notebooks, [jupyterflame](https://github.com/H4dr1en/jupyterflame) looks interesting, supporting flamegraph visualisations of profiled software that _"allow the most frequent code-paths to be identified quickly and accurately"_ _\[what would a flamegraph over a complete notebook look like, I wonder, particularly if it could relate back to the notebook structure?—Ed.\]_  
  
For R'n'Py users, an [announcement](https://blog.rstudio.com/2019/12/20/reticulate-1-14/) on the RStudio blog reveals that _"**reticulate** (v1.1.4) can now automatically configure a Python environment for the user, in coordination with any loaded R packages that depend on reticulate"_. Handy...  
  
Commercially speaking, Astraea Inc.'s [EarthAI notebook](https://astraea.earth/earth-ai-notebook/) is a subscription based _"fully hosted \[GPU Enabled\] and managed JupyterLab Notebook designed specifically to analyse raster data"_, with access to _"8+ PB of free data via Earth OnDemand catalog"_. The company repos suggests they do contribute stuff back to the community _\[though not directly to Jupyter projects?—Ed.\]_, so I'll maybe let them off on the subscription front...;-)  
  
Further south, the _Digital Earth Australia_ notebooks repository, [dea-notebooks](https://github.com/GeoscienceAustralia/dea-notebooks), _"hosts Jupyter Notebooks, Python scripts and workflows for analysing \[Creative Commons licensed\] Digital Earth Australia (DEA) satellite data and derived products"_.  
  
I've previously mentioned the JupyterLab extension,[jupyterlab-nvdashboard](https://github.com/rapidsai/jupyterlab-nvdashboard), for displaying **GPU usage dashboards** but as the [announcement](https://medium.com/rapids-ai/gpu-dashboards-in-jupyter-lab-757b17aae1d5) post recently got recirculated, a couple of extra points jumped out at me in matters arising. First, it provides a **Bokeh Server** application that will display live GPU utilisation, memory and throughput reports as you run your GPU tasks. _\[NVIDIA GPUs only, perhaps, based on its [pynvml](https://pypi.org/project/pynvml/) requirement?—Ed.\]_ Second, it builds on the [jupyterlab-bokeh-server](https://github.com/ian-r-rose/jupyterlab-bokeh-server), a JupyterLab extension _"for displaying the contents of a Bokeh server"_. Third, and in turn, one of the bokeh-server demos uses the _psutil_ Linux module to show CPU, memory, network, and storage activity. Taken together, these should be able to provide folk who care about such things with a handy overview of the performance state of their system.  
  
Perhaps also sys-admin related, [jupyterhub-jobmanager](https://github.com/jcrist/jupyterhub-jobmanager) is an as yet undocumented _\[and already stalled?!—Ed.\]_ work-in-progress that looks as if it may provide a dashboard for monitoring and managing user jobs on a JupyterHub server _\[I'm still JupyterHub-naive so haven't tried this yet...This year I WILL get round to playing with JupyterHub...—Ed.\]_.  
  
Meeting a requirement for LaTeX in a Jupyter environment, for example to provide a **PDF export** route for your notebooks, can add a lot of weight to the environment. The [nbpdfexport](https://github.com/yuvipanda/nbpdfexport) package takes an alternative route, using _pyppeteer_ and a headless Chrome browser to provide a _"PDF via Chrome"_ notebook export option _without the need to install LaTeX_. Which reminds me of [a trick](https://blog.ouseful.info/2019/01/16/converting-pandas-generated-html-data-tables-to-png-images/) I use to grab screenshots of stylised pandas tables / html, which also works for grabbing folium map screenshots etc. _\[Does always seem rather overkill though...—Ed.\]_ Digging around, there's also this old [nbbrowserpdf](https://github.com/Anaconda-Platform/nbbrowserpdf) conda package which also claims to support _"LaTeX-free PDF generation for Jupyter Notebooks"_. Interestingly, after that package creates the PDF, [by default](https://github.com/Anaconda-Platform/nbbrowserpdf/blob/6381c3019f7023a88e29d5342db61f66e8abb95f/nbbrowserpdf/exporters/pdf_capture.py#L170-L180) it will also embed the original _.ipynb_ document in the PDF _\[[via](https://discourse.jupyter.org/t/sustainability-of-the-ipynb-nbformat-document-format/3051/4?u=psychemedia) __Nicholas Bollweg__\]_.   
  
The Jupyter **[ContentsManager](https://jupyter-notebook.readthedocs.io/en/stable/extending/contents.html)** is responsible for translating file interactions into operations on a particular storage medium _\[which is to say, it saves and loads your notebooks for you...—Ed.\]_. The [HybridContentManager](https://github.com/viaduct-ai/hybridcontents) is a fork of Quantopian's original _pgcontents_ content manager that removes the Postgres dependencies and allows you to mix and match different content managers for different directories, allowing you to _"easily move files between different content managers (i.e local files to s3 backed manager)"_. Path validation is enabled to keep naming schemes consistent and/or prevent illegal characters. There's also a new [jupyter-pyfilesystem](https://github.com/manics/jupyter-pyfilesystem) _ContentsManager_ that uses the **[PyFilesystem](https://docs.pyfilesystem.org/en/latest/guide.html)** for storing files. New to me, PyFilesystem _"simplifies behaviour compared to io and os modules"_ and provides yet another level of indirection, allowing you to write to remote locations, such as remote FTP directories, for example, using the same API as you would to write to the local filesystem.  
  
One of the great things about notebooks is they can be used to expose computational stuff to folk who don't normally see it (or why it would be useful), and contextual stuff to the geeks who'd might otherwise ignore it. So a couple of recent **preprints** relating to that: from Lorena Barber, _Engineers Code: reusable open learning modules for engineering computations_; and from my own camp, a [paper](http://oro.open.ac.uk/68285/) to be presented later this year on _Developing Students' Written Communication Skills with Jupyter Notebooks_. And whilst not specifically Jupyter related _per se_, a still relevant [piece](https://www.ijoer.org/a-look-at-the-future-of-open-educational-resources/) from longtime edtech practitioner and commentator, Stephen Downes, that takes _"a look at the future of open educational resources"_.   
  
Following on from that, if you're interested in the future of **publishing HTML sites from Jupyter** runnable documents, this Jupyter Book [issue](https://github.com/jupyter/jupyter-book/issues/460)_exploring using Sphinx, instead of Jekyll, __to build Jupyter Books_is well worth chipping into.   
  
Way back in [#TJ13](https://github.com/TrackingJupyter/archive/blob/master/TJ13.md#widget-architectures), I mentioned not grokking [jp\_proxy\_widget](https://github.com/AaronWatters/jp_proxy_widget). I've since had a play with it _\[h/t [SO](https://stackoverflow.com/questions/59641390/jupyter-widget-to-play-audio-with-playhead-on-graph)...—Ed.\]_ and can see it being really handy as a tool for rapidly prototyping _ipywidgets_ by wrapping third party javascript packages. Here's [a quick example](https://blog.ouseful.info/2020/01/11/rapid-widget-prototyping-using-third-party-javascript-packages-in-jupyter-notebooks/) of using it to trivially wrap an interactive [Wavesurfer.js spectrogram](https://wavesurfer-js.org/example/spectrogram/index.html) in a notebook as an _ipywidget_.  
  
Chart-wise, if you're looking to get started with using Vega-Lite and Altair, [this](https://github.com/uwdata/visualization-curriculum) "visualisation curriculum" from the University of Washington is probably worth a look. Providing both Jupyter and observable notebooks, I wondered if they'd done something clever to produce the one from the other, or both from a common feedstack. But it seems not. _\[The text is more or less the same, although not identical: the Observable notebooks are slightly more polished. —Ed.\]_  
  
Remember when plotly was an online thing and you needed a key and a network connection _\[or do I misremember?!—Ed.\]_? Recapture the experience with this [new Python wrapper](https://blog.datawrapper.de/datawrapper-python-package/) for the [**Datawrapper**](https://www.datawrapper.de/) online chart hosting service _\[that probably comes across as snarky, but it's not meant to; Datawrapper is well used package for helping journalists get charts into stories in a really sensible way. —Ed.\]_  
  
I don't know if it's something in the air, but I've been spotting quite a Jupyter powered **spatial analyses** lately. Many of them are quite pretty, whilst at the same time threatening to kill your browser. So for example, via this [blog post](https://anitagraser.com/2020/01/03/movement-data-in-gis-26-towards-a-template-for-exploring-movement-data/), there's this [notebook template](http://exploration.movingpandas.org/) for exploring movement data. If it's urban analytics that tickles your fancy, here's a [notebook](https://nbviewer.jupyter.org/github/uber/h3-py-notebooks/blob/master/H3%20API%20examples%20on%20Urban%20Analytics.ipynb) demoing how to use the Uber H3 geospatial indexing system _\[that's not much of a tease, is it? But just check it out...Purdy...—Ed.\]_. Elsewhere, and in the form of a Jupyter book in the wild, is this fascinating insight into [urban morphometrics](https://guide.momepy.org/intro).  
  
There's also a new_osmnx_ ([#TJ14](https://github.com/TrackingJupyter/archive/blob/master/TJ14.md#doing-the-geo-thing-in-jupyter-notebooks)) paper by Geoff Boeing _\[spelled correctly this time!—Ed.\] _on _Urban Street Network Analysis in a Computational Notebook_ (downloadable as either a PDF _or_ an _.ipynb_ file....). Describing a situation in which _"researchers act as both code creators and code users"_ and where _"computational notebooks open up the world of analytics to a wider audience than was possible in the past"_, the paper/notebook documents the OSMnx demos/examples/tutorials repository, _"demonstrating OSMnx interactively through a synoptic tutorial adapted from the repository"_.  
  
As a side note, as well as researchers doing the code thing, we're also seeing research software engineers doing the publication dissemination thing. For example, [JOSS](https://joss.theoj.org/) (The **Journal of Open Source Software**), _"a developer friendly, open access journal for research software packages, with a formal peer review process designed to improve the quality of the software submitted, a simple submission workflow and extensive documentation to help you prepare your submission"_. They claim that _"if your software is already well documented then paper preparation should take no more than an hour"_.  
  
In passing, **[ipyleaflet](https://github.com/jupyter-widgets/ipyleaflet)** just bumped up to v0.12. Changes include _"several bug fixes and a modernization of the front-end code"_, apparently, although there isn't _\[yet...?—Ed.\]_ a changelog available _\[other than reviewing the commit history yourself, of course...—Ed.\]_. Also in the _jupyter-widgets_ Github organisation namespace, I note [midicontrols](https://github.com/jupyter-widgets/midicontrols), a Jupyter widget for **midi controllers** that makes it easy to hook up to notebook widgets _\[and hence synched Python kernel state?—Ed.\]_ with hardware controllers _\[I think?!—Ed.\]_.  
  
Related to the whole geo thing, _Tracking Jupyter_ subscriber José L Gómez-Dans got in touch with the message that _"for all of us geospatial folk, **[geonotebook](https://github.com/OpenGeoscience/geonotebook)** looks like a super tool for Jupyter, but alas, looks abandoned and very few people seem to be aware of it"_. He continues, _"\[m\]aybe it's been subsumed on other projects, but it's a really compelling idea!"_. And it certainly looks it, not least because incorporates its own tile server based on Mapnik and GDAL as a Jupyter Notebook server extension. So, does anyone out there know of any follow on projects, or has the capacity to help keep this project ticking over?  
  
Which reminds me... One of the things I need to do this year is start putting together proper curated collections of links on a particular theme. Previous attempts include [this one](https://github.com/innovationOUtside/ioc-infrastructure-review/wiki), but until I get round to doing something properly, here's just another random link on the recurring theme of getting things running in different cloud environments: a [recipe](https://gianniceresa.com/2019/12/jupyter-sandbox-in-an-oracle-cloud-compute-instance/) for creating a JupyterLab sandbox on an **Oracle Cloud** Compute Instance.  
  
A couple of things for the **Bayesian** modellers out there... After seeing several references to "Stan" over the Winter break, I went searching... [Stan](https://mc-stan.org/), it seems, is _"__a state-of-the-art platform for statistical modeling and high-performance statistical computation"_. Wrappers for a wide range of languages — Python, R, Julia and Stata included — are available. The Python wrapper, [PyStan](https://pystan.readthedocs.io/en/latest/), is itself wrapped by the _\[rather old? —Ed.\]_ [jupyterstan](https://github.com/janfreyberg/jupyterstan) block magic with additional _stan_ cell syntax highlighting _\[it seems to work, albeit slowly, on MyBinder...—Ed.\]_. PyStan objects are also readable by [arviz](https://github.com/arviz-devs/arviz) \[[paper](https://www.theoj.org/joss-papers/joss.01143/10.21105.joss.01143.pdf)\], a Python package for exploratory analysis of Bayesian models that also provides built-in support for PyMC3, CmdStanPy, Pyro, NumPyro, emcee and TensorFlow Probability objects. It [looks like](https://github.com/arviz-devs/Exploratory-Analysis-of-Bayesian-Models) there are some notebook based _arviz_ educational materials under way, but when I did a quick web search for them, I didn't see these formally published (as yet) on either their own site, or on the [docs](https://arviz-devs.github.io/arviz/) site.  
  
And finally, I'm not sure why anyone would want this, but if you do, here you are: a [Dockerised PowerShell kernel](https://github.com/pcgeek86/jupyter-powershell)... And also, late breaking, code for this [jupyter-hdfs-kernel](https://github.com/Jasper912/jupyter-hdfs-kernel) seems to have just appeared...

Running JupyterHub and BinderHub Services on GPU and FPGA Enabled Servers
-------------------------------------------------------------------------

It wasn't very long ago when remote hosting meant accessing servers with straightforward, albeit multicore, x86 processors. But now there is far more choice: ARM cores, GPUs and even FPGAs are now available, on demand, at the checking of a credit card. _\[And quantum computers.. I... just...—Ed.\]_  
  
The Zero-to-JupyterHub instructions [provide details](https://zero-to-jupyterhub.readthedocs.io/en/latest/customizing/user-resources.html#set-user-gpu-guarantees-limits) of how to configure your profile to allow Kubernetes to launch kernels on GPU-enabled servers, and there's a deployment story in the repo describing how to [use GPUs on GKE](https://github.com/jupyterhub/zero-to-jupyterhub-k8s/issues/994). The latter includes a nice spawner launch page that shows how a JupyterHub user can select their server type, which I'm guessing is implemented using the [Kubespawner profile list](https://jupyterhub-kubespawner.readthedocs.io/en/latest/spawner.html#kubespawner.KubeSpawner). There are also various recipes (eg [here](https://github.com/FAU-DLM/GPU-Jupyterhub) and also [forked](https://github.com/whlteXbread/GPU-Jupyterhub)) for building GPU supporting JupyterHub Docker containers.  
  
As far as Binderhub goes, the [official](https://binderhub.readthedocs.io/en/latest/zero-to-binderhub/index.html#zero-to-binderhub) Zero-to-Binderhub docs (or The Turing Way Azure deployment zero-to-Binderhub [workshop docs](https://github.com/alan-turing-institute/the-turing-way/blob/master/workshops/build-a-binderhub/workshop-presentations/zero-to-binderhub.md)) don't make any mention of GPUs particularly, but I'm guessing GPU-enabled services can be specified using the Binderhub's Kubespawner profile _\[a UI to allow users to select the Binderhub server appears to have [stalled](https://github.com/jupyterhub/binderhub/issues/731)?—Ed.\]_. An example GPU backed Binderhub deployment, as used at NeurIPS 2018, can be found [here](https://github.com/consideRatio/neurips.mybinder.org-deploy).  
  
But what if you want to run your notebooks against an FPGA? A [recent post](https://docs.inaccel.com/latest/labs/binderhub-aws/) on_ Zero to FPGAs using BinderHub_ describes how to enable FPGA accelerated notebooks using Binderhub on AWS using the _Inaccel FPGA Kubernetes Plugin_ _\[no, me neither...—Ed.\]_ Which rather makes me think: could we end up with kernels that define FPGA configurations and custom instruction sets/code cells designed to make the most of them? _\[Jupyterised Hardware–Software Codesign ftw?! —Ed.\]_

And Yet More Binder-ish Developments
------------------------------------

As a piece of the Jupyter service delivery jigsaw, Binderhub and the idea of building and running environments from simple environment defining repositories seems to me to really be coming into its own. So here are a couple of unofficial projects that really help build on the idea of Binderised repos.  
  
First up, a [thread](https://discourse.jupyter.org/t/a-persistent-binderhub-deployment/2865) started by the GESIS team on the Jupyter discourse community described a working example of a _persistent_ BinderHub deployment that allows users to save files that are associated with Binder run environments to a persistent user-account. This deployment appears to offer user accounts on a BinderHub server against which a persistent volume can be mounted. The persistent BinderHub can then launch an environment/server for a user, and mount their persistent volume against it. From the forum thread, replicating the full authenticated behaviour looks like it may be problematic at the moment, but it'll be interesting to see it appear hopefully as part of the official release, and the Zero-to-BinderHub docs, at some point...  
  
Secondly, the personalised, persistent, Binder desktop application that is_containds_ ([#TJ24](https://github.com/TrackingJupyter/archive/blob/master/TJ24.md#news-and-announcements)) continues to develop apace _\[disclaimer: I've been chatting with Dan Lester, the developer...—Ed.\]_. Derived from Kitematic, **ContainDS** lets you:

*   launch a Jupyter container from eg Jupyter Docker stack, with a single click (the token etc is handled for you so the notebook page just opens in the browser);
*   build a container from a local dir or a Github repository, as per MyBinder, using repo2docker _\[which I think runs itself from a docker container, so no external install required? I need to check that...—Ed.\]_;
*   take snapshots / clones of an image and give them a new image name;
*   export containers as a standalone file (albeit a large one!) that can be shared with others and loaded in using _containds_.

​One workflow I'm [exploring](https://github.com/innovationOUtside/tm129-robotics2020) uses a [Github action](https://github.com/ouseful-PR/repo2docker-action) to build a container from releases in the repo using repo2docker and then push it to dockerhub; I can then distribute the image to others via Dockerhub, the images being pulled using _ContainDS_. _\[FWIW, I also [posted](https://blog.ouseful.info/2020/01/08/binderising-the-tm351-virtual-machine/) a review of how our course virtual machine definition was ported to a Binder container definition.—Ed.\]_  
  
In passing, I also note that if you need to persistently stash files processed in postBuild in a Binderised container, you can pop them into _$CONDA\_DIR_ (and which defaults to _/srv/conda_).  
  
And finally, I note that _repo2docker_ doesn't seem to have had an update for some time, but there have been a couple of merges sitting for some time, just waiting, for their day in the sun. Specifically, [Figshare](https://github.com/jupyter/repo2docker/pull/788) and [dataverse](https://github.com/jupyter/repo2docker/pull/739) content providers are available, in the dev repo at least. Support for using Binder with Dataverse DOIs [is already available](https://github.com/jupyterhub/binderhub/pull/969) _\[see also [this](https://groups.google.com/forum/#!msg/dataverse-community/K7vKWSPLaQY/8go3fvV3AwAJ) dataverse-community discussion...—Ed.\]_: to give it a spin, just go to MyBinder.org, click the dropdown to select "Dataverse DOI", enter the DOI of a Dataverse dataset and launch your container... Zenodo and Figshare DOIs can also be selected. _\[Erm... should you just be able to select "DOI"...?—Ed.\]_

Concerning Cell Execution
-------------------------

In [#TJ24](https://github.com/TrackingJupyter/archive/blob/master/TJ24.md#keeping-track-of-cell-history) I reviewed several extensions that support cell history logging. One thing I missed was the [gannymede logger](http://github.com/Lab41/ganymede_nbextension) extension that logs in a JSON format the inputs and outputs for each run cell in a Jupyter notebook in order to _"accurately reconstruct a user's interactive session"_. A perhaps simpler logging approach is offered by _nbgallery_ \[[#TJvarious](https://github.com/TrackingJupyter/archive/search?q=nbgallery&unscoped_q=nbgallery)\] in the form of its [simple instrumentation support](https://github.com/nbgallery/nbgallery/blob/master/public/integration/gallery-instrumentation.js). I guess the [Jupyter](https://github.com/jupyter/telemetry)[ telemetry project](https://github.com/jupyter/telemetry), which is exploring configurable event-logging for Jupyter applications and extensions, is a more complete generalisation of the whole logging approach?  
  
Within the notebook, I've mentioned the [Jupyter-multi\_outputs](https://github.com/NII-cloud-operation/Jupyter-multi_outputs) in several previous issues of _Tracking Jupyter_ _\[and also posted my own review [here](http://https://blog.ouseful.info/2019/02/11/quick-review-jupyter-multi-outputs-notebook-extension/)...—Ed.\]_ but it's worth flagging again here as a reminder of its support for pinning cell outputs and then diff-ing them against the current output.  
  
When it comes to managing cell execution, the [execution\_dependencies](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/execution_dependencies/README.html) notebook extension, which forms part of the official unofficial notebook extension distribution, is something I haven't really found myself ever using, though it may be useful in some instructional notebooks, particularly ones with locked/frozen code cells. By tagging dependent cells with ID tags, and dependent cells with dependency identifying tags, the extension recursively checks cells when they're run for all dependencies and executes those before running the cell itself. _\[I'm not sure if you have to run a cell first for it to be available in the dependency chain, or whether you can irregularly sort cells and let the tagging handle the execution order? Which could get very messy...—Ed.\]_  
  
The [notebook](https://nbviewer.jupyter.org/github/friggeri/notebooks/blob/master/tracking_inconsistencies_in_notebooks.ipynb) on _"Tracking inconsistencies in Jupyter notebooks"_ provides another example of what might be involved in tracking cell execution state, with a sketch of a simple proof of concept "consistency tracking kernel" that tracks cell execution and dill captured variable state _\[the next issue of Tracking Jupyter will look at capturing environment state in more detail. This edition is already overlong! —Ed.\]_.  
  
Another take on enforced dependent cell execution models, [Wrattler](http://www.wrattler.org/) has been on my watchlist for ages, so it was good to finally give it a quick spin at least _\[I don't know if the "Wellcome to Wrattler" message when you create and open an empty .wrattler file is a hat tip to a a funder, or a typo... The demos also come with typos: Pyhthon, anyone? I also couldn't obviously see how to create a .wrattler doc (eg. nothing in the launcher?) either?—Ed.\]_. A "next generation notebook", it runs as a JupyterLab extension supporting a new _.wrattler_ notebook filetype \[[Binder demo](https://github.com/wrattler/wrattler-binder)\]. Offering polyglot notebooks, you can load in R data packages and then use them as Python pandas dataframes without further ado. Rendering Python matplotlib, Javascript plotly and R ggplot2 charts in the same notebook is similarly a breeze in the demos at least, although I wasted some time trying, and failing, to run myself a simple d3.js demo _\[must be breaking something somewhere, perhaps with the svg axis?—Ed.\]_. I _think_ the docs suggest that a _linear_ dependency chain through the notebook is respected (_"__if you evaluate cell that depends on some cell that you have not evaluated already, Wrattler will automatically evaluate both cells"_) and _I think_ those dependencies are limited to data frames (_"__you can only pass data frames between cells, with everything else being local to its cell"_) but I didn't take the time to explore this. A tool for visualising dependency graphs is provided, although it didn't render in the Binder demo for me. _\[I'm starting to remember why I've kept putting off trying Wrattler out... Maybe in another few months...—Ed.\]_  
  
More generally, if enforcing cell execution order is your thing, see things like _dataflownb_, _reactivepy_, _nodebook_, _streamlit_ and _polynote_, as mentioned variously in previous editions of Tracking Jupyter. For polyglot notebooks, I've previously linked to things like SoS notebooks, Stencila and the BeakerX kernels. 

On Preservation and Reproducibility
-----------------------------------

Concerns about long term preservation of notebooks are surely going to become _a thing_. In my institution, where we use notebooks in distance education teaching, a course production time of 2 years and a course life of 5 years raises a _lot_ of interesting questions around maintenance, particularly given that a course offering is not supposed to change substantively over the presentation period... Maintaining the course archive over 20+ years is another concern. Journals, which is to say, publishing sites of record, are starting to take notice of notebooks too. And on Github? Well, there's now more than a squillionty millionty notebooks _\[or [something like that](https://github.com/parente/nbestimate)...—Ed.\]_.  
  
The archivists _are_ starting to pay attention though, as this [quick summary](http://www.dcc.ac.uk/blog/managing-computational-notebooks-overview-%E2%80%98chopportunities%E2%80%99) of a birds of a feather discussion from a recent [Research Data Alliance](https://www.rd-alliance.org/) (RDA) meeting demonstrates. The review also comments on the use of notebooks _"as digital objects that align with the [FAIR](https://www.go-fair.org/fair-principles/) (Findable - Accessible - Interoperable - Re-usable) principles"_. Interesting... _\[FWIW, I also posted a [recent thing](https://blog.ouseful.info/2020/01/13/sustainability-of-the-ipynb-nbformat-document-format/) about "S__ustainability of the ipynb/nbformat Document Format"__. There's an associated Jupyter discourse forum thread [here](https://discourse.jupyter.org/t/sustainability-of-the-ipynb-nbformat-document-format/).—Ed.\]_  
  
It's not just notebooks, of course. How do we create reproducible computing environments for the notebooks to run in, and what does it mean for them to be reproducible anyway? A [recent](https://f1000research.com/articles/7-742/v2) set of recommendations for the packaging and containerising of bioinformatics software, published via _F1000Research_, identifies simplicity, maintainability, sustainability, reusability, interoperability, user’s acceptability, size, and transparency as areas of concern _\[although you'll have to read the paper to find out what each concern actually involves!—Ed.\]_.  
  
On the question of reproducibility, [Towards reproducible Jupyter notebooks](https://hpc.guix.info/blog/2019/10/towards-reproducible-jupyter-notebooks/) is an example of one line of discussion that creeps cropping up: that of making notebooks “deployment-aware”, with automatically deployable software requirements specified as integral part of the notebook itself _\[which is to say: what if the notebook itself could describe its dependencies and the notebook server install them automatically?—Ed.\]_ The post describes a [Guix kernel](https://gitlab.inria.fr/guix-hpc/guix-kernel) for Jupyter, with magic-like annotations used to define external requirements. Unlike simpler packaging formats, [guix](http://guix.gnu.org/) seems to capture the complete computational environment dependency graph _\[which is to say: everything, and everything is pinned...—Ed.\]_. There's a related discussion about the pros/cons of embedding dependencies in notebooks on the Jupyter discourse site [here](https://discourse.jupyter.org/t/guix-jupyter-towards-self-contained-reproducible-notebooks/2379).  
  
Something to think about for the new year? Maybe get the librarians involved a bit more? _\[It might also be a way in to getting them to run notebook servers too...?!;-) —Ed.\]_  
  
PS via _[The Turing Way newsletter](https://tinyletter.com/TuringWay)_, which like _Tracking Jupyter_ seems to go straight to my spam folder, I note this [virtual half-day conference](https://vickysteeves.gitlab.io/librarians-reproducibility/) on _Librarians Building Momentum for Reproducibility_ on January 28, 2020, 9:00-3:00pm PST / 12:00-6:00pm EST _\[so that'll be 17:00-23:00 UK time, then?—Ed.\]_.

And Finally...
--------------

Nothing to do with Jupyter, but it got me wondering...[AIDungeon2](https://github.com/AIDungeon/AIDungeon) is an GPT-2 powered, generative text adventure game \[[announcement post](https://pcc.cs.byu.edu/2019/11/21/ai-dungeon-2-creating-infinitely-generated-text-adventures-with-deep-learning-language-models/)\]... Hmm.. thinks... you know Tiddlywiki? So what if AIDungeon2 was a bit like that? Display a text cell with the first line of the adventure, and a code cell for your action. Enter you action, and generate a text cell cell with the response, followed by your next action.  
  
A conversational notebook...?!  
  
As the _AIDungeon_ repo strapline suggests: _Infinite adventures await!​_  
 

* * *

Disclaimer: this newsletter is produced independently of the official Jupyter project.

_All errors are down to the editor... Oops.. If I make any that I later become aware of, or I'm informed of, I'll announce them in the first possible issue thereafter. If it's really bad, I'll do a Stop Press/emergency issue._

If you have any Jupyter related news items or notebooks you'd like to be considered for inclusion in the newsletter, or experiences of using any of the technologies described in this newsletter that you'd like to share, please email them along to: tony.hirst@open.ac.uk
