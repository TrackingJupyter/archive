Welcome to this twenty-sixth edition of the _Tracking Jupyter_ newsletter (#TJ26).  
  
It's light on deep dives this issue, because while I have notes on more than several underway, they're in nothing other than note form _\[\*very\* note form...—Ed.\]_ at the moment. Maybe in the next issue...

News
----

First up, [**JupyterCon** is back](https://blog.jupyter.org/jupytercon-2020-is-a-go-47c82b281fa8) for 2020. Running from August 10–14, 2020 at the Berlin Conference Center in, _\[as you probably guessed...—Ed.\]_, Berlin. The CFP will be out soon and a [call for reviewers](https://docs.google.com/forms/d/e/1FAIpQLSdzEoFUh6zb8liiWeP45PjyUtc0rlE-GPSR-UyNRnfR6WqayA/viewform?usp=send_form) is already open.  
  
**IPython** has had its regular update, pootling along to [v.7.12](https://ipython.readthedocs.io/en/stable/whatsnew/version7.html#ipython-7-12) with code cleanup rather than new features. **Jupyterhub** is now at at v1.1.0 \[[release notes](https://jupyterhub.readthedocs.io/en/stable/changelog.html#id1)\], with _"lots of accumulated fixes and improvements, especially in performance, metrics, and customization"_. Baked in support for the Jupyter telemetry project also starts to make an appearance... And there's a Jupyterlab 2.0 RC [out](https://github.com/jupyterlab/jupyterlab/releases/tag/v2.0.0rc0)...  
  
Not strictly Jupyter related, but I'm sure it will be of interest to a lot of Jupyter Python kernel users, **_pandas_** is [now out at v.1.0.0](https://pandas.pydata.org/pandas-docs/version/1.0.0/whatsnew/v1.0.0.html). _\[There's also experimental support for a nullable integer in the form of __pd.NA. Which is really exciting, right...?:-) \[Sigh...—MetaEd.\]—Ed.\]_  
  
Another addition to the Jupyter fold, the **Xeus** C++ implementation of the Jupyter kernel protocol has now been [incorporated](https://blog.jupyter.org/xeus-is-now-a-jupyter-subproject-c4ec5a1bf30b) as a Jupyter sub-project, which means among other things that it will comply with the [Jupyter governance process](https://github.com/jupyter/governance).  
  
Microsoft's .NET developer platform continues to grow it's support for all things Jupyter with the [announcement](https://devblogs.microsoft.com/dotnet/net-interactive-is-here-net-notebooks-preview-2/) of **.NET notebooks preview 2** and Jupyter integration in the _dotnet interactive_ tool. As well as offering official [PowerShell support](https://devblogs.microsoft.com/powershell/public-preview-of-powershell-support-in-jupyter-notebooks/) in Jupyter notebooks, several official .NET kernels are [also available](https://nteract.io/kernels/dotnet) for the nteract desktop app. _\[Hmmm..so as well as Jupyter support in the VS Code Python extension, at least, Microsoft are also mindful of supporting a bespoke (desktop based) notebook client? —Ed.\]_ The February, 2020, [release](https://devblogs.microsoft.com/python/python-in-visual-studio-code-february-2020-release/) of the VSCode Python extension, which provides Jupyter notebook support, has improved startup performance but no real new features of note. If you're already an **nteract** user, there's a [request](https://discourse.jupyter.org/t/request-for-community-feedback-new-nteract-sidebar-design/3136) out for community feedback on the new nteract sidebar design.  
  
Industry dinosaur and behemoth **Oracle** recently [announced](https://www.oracle.com/corporate/pressrelease/oracle-cloud-data-science-platform-021220.html) their _Oracle Cloud Data Science Platform_, with Jupyter in the mix, along with what looks like large amounts of other open-source goodness  _\[personally, I think those projects should team up and look for a way to sue Oracle for breach of API copyright and/or trademark infringement, just because...—Ed.\]_. There's a crappy [free tier](https://www.oracle.com/uk/cloud/free/) but you have to hand over name, address, email and a phone number \[to which a confirm code is sent and has to be resubmitted before you can progress...—Ed.\]. The next step is to provide your credit card number, at which point I bailed (the help chat person _\[or bot???—Ed.\]_ responded that the data I'd submitted up to then would be deleted if and when I bailed _\[we'll see...—Ed.\]_, so I have no idea what the service is like. _\[And I probably won't be mentioning it again...—Ed.\]_.  
  
Pay to play desktop GIS client ArcGIS Pro 2.5 was recently [released](https://www.esri.com/arcgis-blog/products/arcgis-pro/analytics/how-to-create-and-add-python-notebooks-in-arcgis-pro-2-5/) with built in notebook support. _\[Integrated support in free'n'open Jupyter environments is still underserved, methinks...—Ed.\]_ Whilst on the topic of commercial geo-services \[hmmm... maybe I should be tracking paid for Jupyter integrating services as a thing...—Ed.\], here's a [recipe](https://blog.mapillary.com/update/2020/02/06/mapillary-data-in-jupyter.html) for accessing Mapillary data from notebooks.  
  
**[Papermill](https://papermill.readthedocs.io/en/latest/)**, the tool for _"parameterizing and executing Jupyter Notebooks"_ is now at v.2.0.0, moving to Python 3.5+ only and adding support for C# and F# kernels. The _nbconvert_ dependency has been replaced by a new package **[_nbclient_](https://nbclient.readthedocs.io/en/latest/)**, which extracts out _nbconvert_'s _ExecutePreprocessor_ in the form of a _"low-opinion in-memory notebook execution library with all the low-level primitives for cell execution in-place"_ _\[handy "when you’re developing a new notebook execution interface (UI, toolchain, etc)" apparently...—Ed.\]_.  
  
This is a [handy productivity tip](https://habr.com/en/post/439570/): some IPython magic to **add tags to a cell**. _\[I use the active-ipynb tag a lot in a Jupytext content to tag code cells that need commenting out in paired text documents, although what I really want is a simple extension that lets me add style to a cell with a particular tag... It'd also be handy to be able to tag new code cells with a default set of tags (eg active-ipynb). \[And speaking of notebook defaults, being able to configure a notebook to create a markdown cell, rather than code cell, would be handy too...—MetaEd.\] —Ed.\]_  
  
I'm not sure if this is ready for prime time yet _\[or even if it's an MVP yet...—Ed.\]_, but it could be handy for users of JupyterHub **Kubernetes** deployments where pods are automatically deleted or restarted if a memory limit is exceeded: a [memory\_monitor](https://github.com/steelersd/memory_monitor/tree/master/memory_monitor/static) extension to monitor, shut down and then restart a kernel when a defined memory usage threshold is met.  
  
If getting code syntax right really isn't your thing, I recently stumbled across a new **[Blockly extension](https://github.com/aolney/fable-jupyterlab-blockly-extension)** for JupyterLab that implements _"a Blockly palette with Fable tooling"_ _\[__I don't really understand the Fable tooling bit?! [Something](https://olney.ai/category/2019/12/27/jupyterlabblockly.html) that will 'let an extension developer use F# instead of JavaScript or TypeScript to write extensions'?__—Ed.\]_. Code generated from the Blockly edited programme can be passed to code cells, and code mapped back from code cells to Blockly blocks. The extension also offers [support](https://olney.ai/category/2020/01/20/intelliblocks.html) for intelliblocks — _"blocks that use intellisense to call properties and functions on any Python library once \[once that library has been called in the current kernel\]"_.  
  
From the new [JupyterBook gallery](https://jupyterbook.org/features/gallery.html), which showcases examples of JupyterBook powered sites, I learn this [handy trick](https://github.com/jupyter/jupyter-book/network/dependents?package_id=UGFja2FnZS0zMjg0ODMyNzQ%3D) for finding Github repositories with a dependency on a particular package... _\[Running this on the Jupyter Book package turns up all manner of texts published using Jupyter book, which is a really handy look-up...—Ed.\]_   
  
It seems to be that time of year when **internships** are advertised. There are one or two Jupyter related ones at [NERSC](https://www.nersc.gov/research-and-development/internships/) if you know anyone who may be interested? _\[Something I need to get better at is tracking job ads that mention Jupyter in some way...—Ed.\]_ On the other hand, if you fancy "making exabytes of LHC data seamlessly accessible on Jupyter Notebooks", there's a project to do just that at [CERN](https://hepsoftwarefoundation.org/gsoc/2020/proposal_SWAN_RUCIO_integration.html).  
  
Props to the team at Imperial College for their _Running Jupyter notebooks on Imperial College’s compute cluster_ [blog post](https://wwwf.imperial.ac.uk/blog/research-software-engineering/2020/02/16/running-jupyter-notebooks-on-imperial-colleges-compute-cluster/) who spotted one of their academics had published a Kaggle kernel to support a publication and thought it'd be _"helpful to explain how to run similar notebooks across multiple GPUs using Imperial’s on-premise research computing service's [Jupyter Service](https://www.imperial.ac.uk/admin-services/ict/self-service/research-support/rcs/support/applications/jupyter/) and Research Data Store"_. With the academics’ permission, they _"modified the notebook and published it in an [exemplar repository](https://github.com/ImperialCollegeLondon/rcs-pacemakers) alongside some instructions to run it on the compute cluster"_. _\[Absolutely brilliant. Give the team a prize..:-)—Ed.\]_  
  
For anyone who didn't make the five day [**_ipywidgets_** workshop](https://github.com/jupyter-widgets/ipywidgets/issues/2587) in Paris in early January 2020 _\[and even for those who did... —Ed.\]_ there's a write-up [here](https://wolfv.github.io/posts/2020/01/19/ipywidgets-writeup.html). Outputs include drag'n'drop support in ipywidgets, due for release as part of ipywidgets v.8 \[[release plan](https://github.com/jupyter-widgets/ipywidgets/issues/2750)\], and the handy looking [jupyterlab-dynext](https://github.com/wolfv/jupyterlab-dynext) extension thatuses require.js to dynamically load Jupyterlab extensions and in so doing _"removes the need for recompilation of the JupyterLab environment"_ _\[does this mean you can do without nodejs? Whatever the case, this is a Good Thing, I think...—Ed.\]_.  
  
At some point, I may need to start **following the money** associated with Jupyter related projects (which are likely to be grant funded and open), or that champion their Jupyter compatibility (which may be VC funded and perhaps not so open..). For example, data science platform provider [Deepnote](https://www.deepnote.com/) just [announced](https://medium.com/deepnote/deepnote-emerges-from-stealth-with-yc-index-and-accel-leading-our-seed-round-12325281cde0) they've raised $3.8M from YC, Index and Accel _"an enhanced, collaborative and Jupyter-compatible cloud-based notebook"_. _\[Nice if you can get it... I wonder if any code or support will trickle back to support Jupyter core projects? I also note that I tried getting an early invite at the end of August, 2019 and the welcome email had a typo in the first line ("I just saw that you singed up..."). Money well spent by the VCs? —Ed.\]_ Looking back, I notice the Jovian.ml platform raised $450k [last year](https://yourstory.com/2019/06/funding-data-science-startup-jovian).  
  
On the funding note, a [recent suggestion](https://discourse.jupyter.org/t/open-collective-for-jupyter/3352) to open the Jupyter project up to an _OpenCollective_ donation / support route appeared on the forums to complement the "[fiscal sponsor](https://numfocus.org/projects-overview)" relationship with NumFocus. Good plan?  
  
In passing, mention of the Economist's Big Mac Index [repo](https://github.com/TheEconomist/big-mac-data) _\[which really should be Binderised...—Ed.\]_ crossed my feed a couple of times recently. I'm not sure if the new article on it has just appeared, but I did notice from the repo that [historical data](https://github.com/TheEconomist/big-mac-data/commit/4411460587b0e00c4eeb9730b4670c9c765e6354) going back to 1986 appears to have been added; so if you want to look at how the index has evolved over time, now you presumably can...  
  
Something else on the potentially trackable front are best-of-breed all-in-one **datascience containers** offering Jupyter UIs. I haven't really started collecting or evaluating any of these yet, but a couple that caught my eye recently as perhaps worthy of further investigation include Google's [Cloud Datalab](http://github.com/googledatalab/datalab) container and NVAITC's [ai-lab](https://github.com/NVAITC/ai-lab). **_Cloud Datalab_** offers a Jupyter based _"easy-to-use interactive tool for data exploration, analysis, visualization, and machine learning";_ it's [also available](https://cloud.google.com/datalab)_"as a service"_ which is to say pre-hosted on Google infrastructure and you pay for computer, storage, bandwdith etc. _\[This seems to be a thing - paid for services that are actually just hosted containers...—Ed.\] _And **_ai-lab_**? That's a JupyterHub fronted _"all-in-one AI development container for rapid prototyping, compatible with the nvidia-docker GPU-accelerated container runtime"_.  
  
As to why I think these might be interesting to track? For one, they're likely to demonstrate fully worked out integrations, which could be useful to learn from. For another, popular ones may end up as hosted services, or as 1-click deployable containers from your web host. For example, _ai-lab_ pitches itself as _'a lighter and more portable alternative to various cloud provider "Deep Learning Virtual Machines"'_. Which makes me think of the Innovator's Dilemma, disrupters starting small and coming in at the bottom, only to take all the lunch in the end...  
  
And finally, _\[and yet another candidate tracking category: template repos...—Ed.\]_, a handy [**template repo**](https://github.com/fomightez/jupyter-rise_with-hide_code) for running the RISE presentation extension in MyBinder with the [_hide code_](https://github.com/kirbs-/hide_code) extension _\[to hide and reveal code, outputs, etc at a cell level ...—Ed.\]_. The extension also provides an _export to PDF_ button that respects hidden code elements _\[though the PDF exports seem a little bit temperamental at the moment...—Ed.\]_.  
 

From the Jupyter Discourse Forums
---------------------------------

One of the oft-made criticisms of the whole Jupyter-thang is that the JSON-based _.ipynb_ document format doesn't diff well, particularly if you also have populated output cells _\[I'm a huge fan of Jupytext, partly for this reason...—Ed.\]_. There's an [interesting discussion](https://discourse.jupyter.org/t/should-jupyter-recommend-a-text-based-representation-of-the-notebook/3273?u=psychemedia) on the official Jupyter discourse site about whether there should be a recommended, if not official, **Jupyter text format**, what flavour it might take, and what it should support. Along the way, I learned about [nbexplode](https://github.com/takluyver/nbexplode), a rather dated, but still interesting, experiment that decomposes a notebook into a directory of files, that can then be recombined back into _.ipynb_ form _\[see also [sqlbiter](https://github.com/thombashi/sqlitebiter/issues/51), which decompose notebooks across several SQLite database tables. —Ed.\]_.  
  
The following _\[which just missed the cut for the previous issue of __Tracking Jupyter__ —Ed.\]_ demonstrates how to cope with a workflow issue that may arise when working with remote servers, or if you overload your own local server: specfically, how do you save a notebook if the server has died? One great answer comes in the form of the [jupyter-offlinenotebook](https://github.com/manics/jupyter-offlinenotebook), a rather handy extension that allows you to save and **load notebooks to local-storage** _"even if you've lost your connection to the server"_. Which means if your MyBinder instance rather inconveniently dies on you, you have a handy way of saving the notebook from its browser view even if the server it was running on is no longer there. The extension is also now [preloaded](https://discourse.jupyter.org/t/getting-your-notebook-after-your-binder-has-stopped/3268?u=psychemedia) into MyBinder notebooks _\[though maybe not Dockerfile built instances?—Ed.\]_, with the MyBinder team looking to provide a similar sort of extension for RStudio environments too... _\[Now i'm just waiting for a server free notebook that runs against pyodide cf. epiphany or observeable.js ...—Ed.\]_  
  
...speaking of MyBinder _\[check running repos frequently for new toolbar buttons...;-)—Ed.\]_, the latest [v0.11.0 release](https://discourse.jupyter.org/t/releasing-repo2docker-v0-11-0/3253/3?u=psychemedia) of **repo2docker** will now _"automatically install RStudio for you when we detect that you are using R packages in your environment.yml"_ _\[I'm not sure you can opt out of this yet...?—Ed.\]_. The [changelog](https://repo2docker.readthedocs.io/en/latest/changelog.html) also notes the addition of Figshare and Dataverse repository support, Julia (1.3), R (IRkernel 1.02; RStudio 1.2) and Python (3.8) updates.  
  
If you do still have a server connection, I've previously mentioned the _nbzip_ extension \[[#TJ13](https://github.com/TrackingJupyter/archive/blob/master/TJ13.md#better-by-default)\] that allows you to zip and download notebooks from a the Jupyter notebook server homepage. For JupyterLab, there's the **[jupyter-archive](https://github.com/hadim/jupyter-archive)** extension which allows you to _"archive and download in a non-blocking way a selected or current folder as an archive (zip, tar.gz, tar.bz2, tar.xz formats available)"_. Decompressing archive files directly in file browser is also supported.  
  
The **persistent Binderhub** deployment ([#TJ25](https://github.com/TrackingJupyter/archive/blob/master/TJ25.md#and-yet-more-binder-ishdevelopments)) being developed by the GESIS folks, which gives you a persistent user account with storage mounted against Binder run environments, has made it [into production](https://discourse.jupyter.org/t/a-persistent-binderhub-deployment/2865/15?u=psychemedia), with a promise of improved documentation coming soon...  
  
Meanwhile, if you're planning a **Kubernetes** deployment to manage your JupyterHub instances, and are wondering how much it'll cost per month, [this thread](https://discourse.jupyter.org/t/cost-of-running-a-jupyterhub-on-kubernetes/3296?u=psychemedia) could be a good place to find and back-of-the-envelope cost calculations.

Around and About
----------------

One of the many things I try to maintain a cursory watching eye over is the extent to which notebooks are appearing in the wider community.  
  
For example, a [recent post](https://searchengineland.com/learn-how-to-chart-and-track-google-trends-in-data-studio-using-python-329119) on the SearchEngineLand blog, a blog for SEO folk, included a _pytrends_ analysis of Google Trends data and a separate example of uploading data to Google Spreadsheets from Python via the _gspread_ package. From there, it can be loaded into Google Data Studio _\["a web-based application for creating reports and dashboards", apparently...—Ed.\]_. A companion notebook on Github is linked to _\[by the by, the code in the blog post wasn't in a code sensitive display and as such layout was borked, but baby steps...!—Ed.\]_.  
  
There was no explicit mention of _how_ to run a notebook, which suggests the SearchEngineLand folk assume their readership knows what notebooks are _and_ how to run them (perhaps the expectation is that SEO folk know about Google's [Cloud Datalab](https://cloud.google.com/datalab) service? Searching the site turned up quite a few other Python based analyses that also used notebooks as the coding environment. What intrigued me particularly is that by having a "user-friendly" environment for running code (in the form of notebooks), it seems that more folk are discovering and sharing code solutions, and as such more people are finding value from them?  
  
Also out there, I notice an increasing trend in commercial data warehousing and integration services such as [panoply.io](https://panoply.io) offering [documentation support](https://panoply.io/docs/connecting-to-panoply/jupyter/) for "integrating" with Jupyter notebooks _\[which is to say, showing how to connect to the service from a Jupyter notebook...—Ed.\]_ Connection magics and extension based integrations are a likely next step in a more bespoke integration offering.  
  
With notebook servers starting to appear everywhere, and more an more connectors available for getting data out of remote data sources and into notebooks, data management concerns over governance and privacy are also increasingly likely to be "a thing" that needs to be (centrally) managed. This is another area where we'll see more and more commercial service offerings, I think, such as in the form of privacy and access managing integration tools and services. One [example](https://blog.privacera.com/data-access-governance-for-jupyterhub-on-aws-with-privacera-a0d0a5f34f10) of such an offering seems to come from an outfit called Privacera, in the form of a _"centralized data access governance platform based on Apache Ranger"_ accessed via a JupyterHub integrated Apache Spark park plugin. Apparently. _\[Thinking that through and drawing pretty PowerPoint slides to illustrate it would make me a "proper" industry analyst, right?!;-) —Ed.\]_

Managing Notebook Packages and State
------------------------------------

A recurring theme in Jupyter commentaries are issues that have been termed, as in [this post](https://www.kdnuggets.com/2019/11/notebook-anti-pattern.html) as, _"notebook anti-patterns"_, such as claimed weaknesses in the contexts of _continuous integration_, _testing_, _version control_, _collaboration_, _state management_, _engineering standards_ and _package management__\[we really need a full checklist, along with examples of why certain things are no longer true, or what extensions can address them...—Ed.\]_.  
  
So as _in many editions of Tracking Jupyter previously, here are _several items relating to effective notebook management _\[with plenty more to come in upcoming editions of Tracking Jupyter...—Ed.\]_.  
  
I often use some_%pip install_ magic at the start of a notebook to pre-emptively install any uncommon required packages that I'm using in the notebook. But this has the disadvantage of being _IPython_ rather than _Python_ code. So to achieve a similar effect — install any missing package into current Python environment — in pure Python, [ipydeps](https://github.com/nbgallery/ipydeps) looks like it should do the job... _\[It does of course require that ipydeps is installed first. Erm...—Ed.\]_  
  
If you're lazy in the way you define package import statements into a notebook, [pyforest](https://github.com/8080labs/pyforest/) might help. Call package name prefixed functions from popular data science packages just anyway using conventional shorthand (_pd_, _plt_, _sns_ etc.) and it will add the import statement for the appropriate package to the top of the notebook for you, as well as loading it in.  
  
If on the other hand you just want to tidy all your import statements into a nice sort order at the top of a notebook, the [isort](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/code_prettify/README_isort.html) official unofficial _nbextension_ may help... _\[This extension makes use of the handy looking [KernelExecOnCells](https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/nbextensions/code_prettify/README.html#kernelexeconcells-library-and-nbextensions) function, "a shared library for creating Jupyter nbextensions which transform code cell text using calls to the active kernel". —Ed.\]_  
  
I've already mentioned the _jupyter-offline_ notebook for saving a notebook when your server has died _\[which also reminds me of this [handy recipe](https://stackoverflow.com/questions/28972614/ipython-notebook-convert-an-html-notebook-to-ipynb/47138762#47138762) for converting an nbconvert'ed HTML view of a notebook back to an .ipynb form, which would be nice to see as (part of) a maintained package somewhere...—Ed.\]_, but what if you want to capture state from a running a server so you can restart a session at the same place you left it at a later date? The[%store magic](https://ipython.readthedocs.io/en/stable/config/extensions/storemagic.html) _\[inspired by the old, old [ipycache](https://github.com/rossant/ipycache) magic, I wonder? —Ed.\]_ let's you store _\[pickled? —Ed.\]_ variable state in IPython’s database so you can restore it at a later date; I'm not sure if there's a[dill](https://dill.readthedocs.io/en/latest/index.html) equivalent anywhere that tries to save the whole notebook environment state? _\[I also wonder if there's a way of saving state into notebook metadata? —Ed.\]_  
  
A more recent [%vault](https://github.com/krassowski/data-vault) magic, [originally developed](https://www.reddit.com/r/IPython/comments/e7wmp9/vault_magic_as_a_zipbased_data_frame_oriented_and/) to support the transfer of dataframe state between notebooks. offers the promise of "_simple, organized, compressed and encrypted storage & transfer of files between notebooks"_ via external zip files _\[as did the old ipycache magic? —Ed.\]_ rather than via the IPython cache _\[though once again, it doesn't seem to try to go for the whole environment state dill thing? —Ed.\]_.  
  
Saving state into and out of notebooks is one thing, but what about a simple mechanic for getting commonly used data sets into a notebook context? [Repo2Data](https://github.com/SIMEXP/Repo2Data) is a Python package for automatically fetching data from remote locations. A JSON file specifies the data source , either as a URL or as a scripted source (eg using _tf.keras.dataset_ or _mnist.load\_data_), downloaded to a specified location, and then unzipped if required. Support for [DataLad](https://www.datalad.org/) _\[a data discovery portal and metadata format, I think? Is that also something like the Open Knowledge Foundation / OKF "frictionless data" packaging model? —Ed.\] _is built-in somehow _\[I haven't made time to play with this yet... —Ed.\]_. I'm not sure if repo2data provides coverage over Kaggle, too? If not, the _\[appallingly named...—Ed.\]_ [@kaggle/jupyterlab](https://github.com/Kaggle/jupyterlab) official Kaggle extension for JupyterLab enables you to browse and download Kaggle Datasets for use in a JupyterLab instance _\[__Kaggle API token required...—Ed.\]_.   
 

* * *

Disclaimer: this newsletter is produced independently of the official Jupyter project.

_All errors are down to the editor... Oops.. If I make any that I later become aware of, or I'm informed of, I'll announce them in the first possible issue thereafter. If it's really bad, I'll do a Stop Press/emergency issue._

If you have any Jupyter related news items or notebooks you'd like to be considered for inclusion in the newsletter, or experiences of using any of the technologies described in this newsletter that you'd like to share, please email them along to: tony.hirst@open.ac.uk
